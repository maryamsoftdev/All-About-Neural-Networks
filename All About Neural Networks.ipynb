{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7806c29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.3\n"
     ]
    }
   ],
   "source": [
    "#Basic neuron code\n",
    "inputs=[1.2,1.3,2.4]\n",
    "weights=[2,3,5]\n",
    "bias=3\n",
    "output=inputs[0]*weights[0]+inputs[1]*weights[1]+inputs[2]*weights[2]+bias\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97e13bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.256\n"
     ]
    }
   ],
   "source": [
    "#Basic neuron code with 4 inputs\n",
    "inputs=[1.2,1.3,2.4,3.3]\n",
    "weights=[0.5,3.0,0.15,0.12]\n",
    "bias=1\n",
    "output=inputs[0]*weights[0]+inputs[1]*weights[1]+inputs[2]*weights[2]+inputs[3]*weights[3]+bias\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1944e3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.256, 6.324999999999999, 13.85]\n"
     ]
    }
   ],
   "source": [
    "#3 neurons with 4 inputs\n",
    "inputs=[1.2,1.3,2.4,3.3]\n",
    "weights1=[0.5,3.0,0.15,0.12]\n",
    "weights2=[2.0,0.83,-0.5,0.62]\n",
    "weights3=[1.7,3.5,1.5,0.2]\n",
    "bias=[1,2,3]\n",
    "output=[inputs[0]*weights1[0]+inputs[1]*weights1[1]+inputs[2]*weights1[2]+inputs[3]*weights1[3]+bias[0],\n",
    "         inputs[0]*weights2[0]+inputs[1]*weights2[1]+inputs[2]*weights2[2]+inputs[3]*weights2[3]+bias[1],\n",
    "         inputs[0]*weights3[0]+inputs[1]*weights3[1]+inputs[2]*weights3[2]+inputs[3]*weights3[3]+bias[2]]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fbb6f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.256, 6.324999999999999, 13.85]\n"
     ]
    }
   ],
   "source": [
    "#3 neurons with 4 inputs\n",
    "inputs=[1.2,1.3,2.4,3.3]\n",
    "weights=[[0.5,3.0,0.15,0.12],\n",
    "         [2.0,0.83,-0.5,0.62],\n",
    "         [1.7,3.5,1.5,0.2]]\n",
    "biases=[1,2,3]\n",
    "layer_outputs = []\n",
    "\n",
    "for neuron_weights, neuron_bias in zip(weights, biases):\n",
    "    neuron_output = 0\n",
    "    for n_input, weight in zip(inputs, neuron_weights):\n",
    "        neuron_output += n_input * weight\n",
    "    neuron_output += neuron_bias  # Adding bias outside the inner loop\n",
    "    layer_outputs.append(neuron_output)\n",
    "\n",
    "print(layer_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be0d8e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6.256  6.325 13.85 ]\n"
     ]
    }
   ],
   "source": [
    "#Basic neuron code with 4 inputs with numpy\n",
    "import numpy as np\n",
    "\n",
    "inputs = np.array([1.2, 1.3, 2.4, 3.3])\n",
    "weights = np.array([[0.5, 3.0, 0.15, 0.12],\n",
    "                    [2.0, 0.83, -0.5, 0.62],\n",
    "                    [1.7, 3.5, 1.5, 0.2]])\n",
    "biases = np.array([1, 2, 3]) \n",
    "output = np.dot(weights, inputs) + biases \n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49d55838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.256  6.325 13.85 ]\n",
      " [ 6.256  6.325 13.85 ]\n",
      " [ 6.256  6.325 13.85 ]]\n"
     ]
    }
   ],
   "source": [
    "#Basic neuron code for 3 examples and 4 features at input with 3 neurons and 4 inputs as there is 4 features with numpy\n",
    "import numpy as np\n",
    "\n",
    "inputs = np.array([[1.2, 1.3, 2.4, 3.3],\n",
    "                 [1.2, 1.3, 2.4, 3.3],\n",
    "                  [1.2, 1.3, 2.4, 3.3]])\n",
    "weights = np.array([[0.5, 3.0, 0.15, 0.12],\n",
    "                    [2.0, 0.83, -0.5, 0.62],\n",
    "                    [1.7, 3.5, 1.5, 0.2]])\n",
    "biases = np.array([1, 2, 3]) \n",
    "output = np.dot( inputs,weights.T) + biases \n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9d4804e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 output:\n",
      "[[ 6.256  6.325 13.85 ]\n",
      " [ 6.256  6.325 13.85 ]\n",
      " [ 6.256  6.325 13.85 ]]\n",
      "\n",
      "Layer 2 output:\n",
      "[[ 6.1456 14.1749 22.2042]\n",
      " [ 6.1456 14.1749 22.2042]\n",
      " [ 6.1456 14.1749 22.2042]]\n"
     ]
    }
   ],
   "source": [
    "#two layer, 1 layer 1 with 4 neoron and layer 2 with 3 neurons, weight depend upon number of neurons\n",
    "import numpy as np\n",
    "\n",
    "inputs = np.array([[1.2, 1.3, 2.4, 3.3],\n",
    "                   [1.2, 1.3, 2.4, 3.3],\n",
    "                   [1.2, 1.3, 2.4, 3.3]])\n",
    "\n",
    "weights1 = np.array([[0.5, 3.0, 0.15, 0.12],\n",
    "                     [2.0, 0.83, -0.5, 0.62],\n",
    "                     [1.7, 3.5, 1.5, 0.2]])\n",
    "biases1 = np.array([1, 2, 3]) \n",
    "\n",
    "weights2 = np.array([[0.1, 0.2, 0.3],\n",
    "                     [0.4, 0.5, 0.6],\n",
    "                     [0.7, 0.8, 0.9]])\n",
    "biases2 = np.array([0.1, 0.2, 0.3]) \n",
    "\n",
    "layer1_output = np.dot(inputs, weights1.T) + biases1 \n",
    "layer2_output = np.dot(layer1_output, weights2.T) + biases2\n",
    "\n",
    "print(\"Layer 1 output:\")\n",
    "print(layer1_output)\n",
    "print(\"\\nLayer 2 output:\")\n",
    "print(layer2_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "91ac2d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 1: [[0.22932314 1.0136021  0.21271895 0.38800308 0.10216035]\n",
      " [0.22932314 1.0136021  0.21271895 0.38800308 0.10216035]\n",
      " [0.22932314 1.0136021  0.21271895 0.38800308 0.10216035]]\n",
      "layer 2: [[ 0.09478982 -0.0834259 ]\n",
      " [ 0.09478982 -0.0834259 ]\n",
      " [ 0.09478982 -0.0834259 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "X = np.array([[1.2, 1.3, 2.4, 3.3],\n",
    "                   [1.2, 1.3, 2.4, 3.3],\n",
    "                   [1.2, 1.3, 2.4, 3.3]])\n",
    "class layer_dense:\n",
    "    def __init__(self,n_inputs, n_neurons):\n",
    "        self.weights= 0.10*np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases=np.zeros((1,n_neurons))\n",
    "    def forward(self,inputs):\n",
    "        self.ouput=np.dot(inputs, self.weights) + self.biases\n",
    "        \n",
    "        \n",
    "layer1=layer_dense(4,5)  #object1\n",
    "layer2=layer_dense(5,2)  #abject2\n",
    "\n",
    "layer1.forward(X)\n",
    "layer2.forward(layer1.ouput)\n",
    "    \n",
    "    \n",
    "print('layer 1:',layer1.ouput)\n",
    "print('layer 2:',layer2.ouput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24943fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 1: [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [-8.35815910e-04 -7.90404272e-04 -1.33452227e-03  4.65504505e-04\n",
      "   4.56846210e-05]\n",
      " [-2.39994470e-03  5.93469958e-05 -2.24808278e-03  2.03573116e-04\n",
      "   6.10024377e-04]\n",
      " ...\n",
      " [ 1.13291524e-01 -1.89262271e-01 -2.06855070e-02  8.11079666e-02\n",
      "  -6.71350807e-02]\n",
      " [ 1.34588361e-01 -1.43197834e-01  3.09493970e-02  5.66337556e-02\n",
      "  -6.29687458e-02]\n",
      " [ 1.07817926e-01 -2.00809643e-01 -3.37579325e-02  8.72561932e-02\n",
      "  -6.81458861e-02]]\n",
      "After Activation Fuction: [[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 4.65504505e-04\n",
      "  4.56846210e-05]\n",
      " [0.00000000e+00 5.93469958e-05 0.00000000e+00 2.03573116e-04\n",
      "  6.10024377e-04]\n",
      " ...\n",
      " [1.13291524e-01 0.00000000e+00 0.00000000e+00 8.11079666e-02\n",
      "  0.00000000e+00]\n",
      " [1.34588361e-01 0.00000000e+00 3.09493970e-02 5.66337556e-02\n",
      "  0.00000000e+00]\n",
      " [1.07817926e-01 0.00000000e+00 0.00000000e+00 8.72561932e-02\n",
      "  0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "#adding activation function\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "'''\n",
    "def spiral_data(points, classes):\n",
    "X = np.zeros((points*classes, 2))\n",
    "y = np.zeros(points*classes, dtype='uint8')\n",
    "for class_number in range(classes):\n",
    "ix = range(points*class_number, points*(class_number+1))\n",
    "r = np.linspace(0.0, 1, points)  # radius\n",
    "t = np.linspace(class_number*4, (class_number+1)*4, points) + np.random.randn(points)*0.2\n",
    "X[ix] = np.c_[r*np.sin(t*2.5), r*np.cos(t*2.5)]\n",
    "y[ix] = class_number\n",
    "return X, y\n",
    "'''\n",
    "nnfs.init()\n",
    "\n",
    "X, y = spiral_data(100, 3)   \n",
    "'''\n",
    "X = np.array([[1.2, 1.3, 2.4, 3.3],\n",
    "              [1.2, 1.3, 2.4, 3.3],\n",
    "             [1.2, 1.3, 2.4, 3.3]])\n",
    "'''\n",
    "class layer_dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.10 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "class Activation_ReLU:  \n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)  \n",
    "\n",
    "layer1 = layer_dense(2, 5)  # object1\n",
    "Activation1 = Activation_ReLU()\n",
    "\n",
    "layer1.forward(X)\n",
    "print('layer 1:', layer1.output)\n",
    "Activation1.forward(layer1.output)\n",
    "print('After Activation Fuction:', Activation1.output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "498459d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp_values: [[1.21510418e+02 2.45325302e+01 2.45960311e+00]\n",
      " [7.33197354e+03 6.70320046e-01 1.08816136e+06]\n",
      " [8.12405825e+02 9.89712906e+03 4.08677144e-03]]\n",
      "norm_values: [[8.18237915e-01 1.65199386e-01 1.65626994e-02]\n",
      " [6.69284683e-03 6.11888378e-07 9.93306541e-01]\n",
      " [7.58581511e-02 9.24141467e-01 3.81601061e-07]]\n"
     ]
    }
   ],
   "source": [
    "#For Softmax\n",
    "import numpy as np\n",
    "import math\n",
    "import nnfs\n",
    "nnfs.init\n",
    "layer_outputs=[[4.8,3.2,0.9],\n",
    "              [8.9,-0.4,13.9],\n",
    "              [6.7,9.2,-5.5]]\n",
    "\n",
    "#E=math.e\n",
    "\n",
    "exp_values =np.exp(layer_outputs)\n",
    "print('exp_values:',exp_values)\n",
    "\n",
    "norm_values=(exp_values/np.sum(exp_values, axis=1, keepdims=True))\n",
    "print('norm_values:',norm_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "936baef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.33331734 0.33331832 0.33336434]\n",
      " [0.3332888  0.33329153 0.33341965]\n",
      " [0.33325943 0.33326396 0.33347666]\n",
      " [0.33323312 0.33323926 0.33352762]\n",
      " [0.33328417 0.33328718 0.33342862]\n",
      " [0.33318216 0.33319145 0.33362636]\n",
      " [0.3331828  0.33319202 0.33362517]\n",
      " [0.33314925 0.33316055 0.3336902 ]\n",
      " [0.3331059  0.33311984 0.33377427]\n",
      " [0.33308133 0.3330968  0.33382186]\n",
      " [0.33311024 0.33312503 0.33376473]\n",
      " [0.33305368 0.33307084 0.33387548]\n",
      " [0.33300948 0.33302936 0.3339612 ]\n",
      " [0.33301342 0.33303306 0.33395353]\n",
      " [0.33299848 0.33301902 0.3339825 ]\n",
      " [0.33312678 0.33318865 0.3336846 ]\n",
      " [0.3329409  0.33296496 0.33409408]\n",
      " [0.3329943  0.33303145 0.33397427]\n",
      " [0.33287367 0.33290187 0.33422443]\n",
      " [0.3328757  0.33290374 0.33422053]\n",
      " [0.33309552 0.33318356 0.33372095]\n",
      " [0.33317602 0.33328283 0.3335411 ]\n",
      " [0.332894   0.33293876 0.33416727]\n",
      " [0.33256376 0.33270204 0.3347342 ]\n",
      " [0.33281362 0.33284548 0.3343409 ]\n",
      " [0.33314452 0.33327034 0.33358508]\n",
      " [0.3331285  0.333258   0.33361349]\n",
      " [0.33241624 0.33257595 0.33500782]\n",
      " [0.33234823 0.33250993 0.3351418 ]\n",
      " [0.3327268  0.3329012  0.33437204]\n",
      " [0.3325126  0.33269662 0.33479086]\n",
      " [0.33303845 0.33320293 0.3337586 ]\n",
      " [0.33289513 0.33302    0.33408487]\n",
      " [0.3324894  0.33269092 0.3348196 ]\n",
      " [0.33217457 0.332373   0.33545247]\n",
      " [0.33198893 0.3321154  0.3358957 ]\n",
      " [0.3319736  0.33208773 0.33593866]\n",
      " [0.33205435 0.3322675  0.33567816]\n",
      " [0.3319392  0.33214495 0.33591586]\n",
      " [0.33208537 0.33231777 0.3355969 ]\n",
      " [0.33213225 0.33237374 0.33549398]\n",
      " [0.33214605 0.33239445 0.33545953]\n",
      " [0.33266544 0.3326531  0.33468142]\n",
      " [0.33167845 0.33184558 0.33647594]\n",
      " [0.3316745  0.33181593 0.33650956]\n",
      " [0.33160535 0.3318128  0.33658186]\n",
      " [0.33236307 0.33234516 0.33529174]\n",
      " [0.3315269  0.33173934 0.33673382]\n",
      " [0.33195725 0.3319697  0.33607304]\n",
      " [0.33166936 0.3317521  0.33657855]\n",
      " [0.33258075 0.33256683 0.33485246]\n",
      " [0.33208066 0.33205757 0.3358618 ]\n",
      " [0.33174273 0.33178118 0.3364761 ]\n",
      " [0.33269423 0.33268243 0.33462337]\n",
      " [0.333272   0.33327085 0.33345714]\n",
      " [0.33312216 0.33311826 0.33375955]\n",
      " [0.33332214 0.33332282 0.33335507]\n",
      " [0.3328328  0.33282354 0.33434364]\n",
      " [0.33328426 0.33328333 0.3334324 ]\n",
      " [0.33317587 0.33317298 0.33365116]\n",
      " [0.3332086  0.33321628 0.33357516]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33270696 0.33269536 0.33459768]\n",
      " [0.3323478  0.33240813 0.3352441 ]\n",
      " [0.33259606 0.3326412  0.33476272]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.3318191  0.33191153 0.33626938]\n",
      " [0.33248538 0.3325373  0.33497733]\n",
      " [0.33177653 0.33187157 0.33635193]\n",
      " [0.33155695 0.33166528 0.33677778]\n",
      " [0.33158022 0.33168715 0.3367326 ]\n",
      " [0.33162925 0.3317332  0.33663756]\n",
      " [0.33193487 0.3320203  0.3360448 ]\n",
      " [0.33149302 0.33160523 0.33690175]\n",
      " [0.3314825  0.33159536 0.3369221 ]\n",
      " [0.33145416 0.33156872 0.3369771 ]\n",
      " [0.3313778  0.33149695 0.33712518]\n",
      " [0.3314144  0.33153135 0.33705425]\n",
      " [0.33160868 0.33171389 0.3366774 ]\n",
      " [0.33130187 0.3314256  0.33727252]\n",
      " [0.33226615 0.33257416 0.33515966]\n",
      " [0.33208826 0.33235922 0.33555248]\n",
      " [0.33124658 0.3313737  0.33737978]\n",
      " [0.33208755 0.33255216 0.3353603 ]\n",
      " [0.3312451  0.33174694 0.33700797]\n",
      " [0.3319183  0.33240187 0.33567983]\n",
      " [0.3322925  0.33264574 0.3350618 ]\n",
      " [0.33178163 0.33200505 0.33621335]\n",
      " [0.33118954 0.33171433 0.33709612]\n",
      " [0.33259884 0.33302283 0.33437827]\n",
      " [0.33048248 0.33100846 0.33850905]\n",
      " [0.33145377 0.33198744 0.3365588 ]\n",
      " [0.33027318 0.3307999  0.33892694]\n",
      " [0.33067098 0.33122468 0.3381043 ]\n",
      " [0.3300957  0.33062175 0.33928254]\n",
      " [0.33054543 0.33110923 0.33834532]\n",
      " [0.32982445 0.3300874  0.34008813]\n",
      " [0.32997224 0.330512   0.33951575]\n",
      " [0.32966128 0.33014154 0.3401972 ]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33331198 0.33331785 0.33337015]\n",
      " [0.3332824  0.33329433 0.33342326]\n",
      " [0.33322895 0.3332356  0.33353543]\n",
      " [0.33320716 0.3332304  0.3335625 ]\n",
      " [0.33323795 0.33323616 0.33352587]\n",
      " [0.33310738 0.3331322  0.3337604 ]\n",
      " [0.33307022 0.3331011  0.33382872]\n",
      " [0.33325028 0.33324873 0.333501  ]\n",
      " [0.3332064  0.33320403 0.33358958]\n",
      " [0.33328602 0.33328512 0.33342886]\n",
      " [0.33315045 0.33314705 0.3337025 ]\n",
      " [0.33291462 0.33294156 0.33414382]\n",
      " [0.33330572 0.33330742 0.33338687]\n",
      " [0.3329965  0.33299026 0.33401325]\n",
      " [0.33324876 0.3332472  0.33350405]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33308855 0.33308402 0.3338275 ]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33323956 0.3332453  0.33351514]\n",
      " [0.33332106 0.3333218  0.33335707]\n",
      " [0.3329886  0.3329822  0.33402923]\n",
      " [0.33322966 0.333236   0.33353436]\n",
      " [0.3329142  0.3329399  0.33414587]\n",
      " [0.3333258  0.33332565 0.33334854]\n",
      " [0.3331667  0.3331769  0.3336564 ]\n",
      " [0.3331867  0.33319572 0.3336176 ]\n",
      " [0.33265233 0.33269402 0.33465365]\n",
      " [0.33274367 0.3327798  0.33447653]\n",
      " [0.33273575 0.33277237 0.33449188]\n",
      " [0.332995   0.33301577 0.33398923]\n",
      " [0.33259594 0.33264112 0.3347629 ]\n",
      " [0.33280396 0.33283642 0.33435968]\n",
      " [0.33256438 0.33261147 0.3348241 ]\n",
      " [0.33264107 0.33268347 0.33467546]\n",
      " [0.33245182 0.3325058  0.33504242]\n",
      " [0.3324333  0.3324884  0.33507833]\n",
      " [0.33240175 0.33245876 0.3351395 ]\n",
      " [0.33240733 0.332464   0.33512866]\n",
      " [0.33240822 0.33246484 0.33512688]\n",
      " [0.3324243  0.33247995 0.33509573]\n",
      " [0.33247864 0.33253098 0.33499038]\n",
      " [0.33243686 0.33249173 0.3350714 ]\n",
      " [0.33259025 0.33270478 0.33470494]\n",
      " [0.3324976  0.33275095 0.33475143]\n",
      " [0.3328265  0.33301556 0.33415794]\n",
      " [0.332275   0.33233973 0.33538526]\n",
      " [0.33296087 0.3331967  0.33384246]\n",
      " [0.33222917 0.33229667 0.33547413]\n",
      " [0.33252788 0.3326719  0.33480024]\n",
      " [0.3327689  0.33297873 0.3342524 ]\n",
      " [0.33182636 0.33212614 0.3360475 ]\n",
      " [0.33261582 0.33280617 0.334578  ]\n",
      " [0.33265924 0.33286625 0.33447453]\n",
      " [0.33273673 0.33302188 0.3342414 ]\n",
      " [0.33294362 0.33321092 0.33384547]\n",
      " [0.33143902 0.33175167 0.3368093 ]\n",
      " [0.33292198 0.333198   0.33388   ]\n",
      " [0.332888   0.33317685 0.33393514]\n",
      " [0.3320177  0.33236402 0.3356183 ]\n",
      " [0.33186933 0.33222404 0.33590665]\n",
      " [0.3312577  0.33137324 0.33736908]\n",
      " [0.33099663 0.3312369  0.33776644]\n",
      " [0.33104226 0.33136436 0.33759338]\n",
      " [0.33169854 0.33207747 0.336224  ]\n",
      " [0.33099547 0.33116755 0.33783698]\n",
      " [0.33116215 0.33153674 0.33730114]\n",
      " [0.33137318 0.33176714 0.3368597 ]\n",
      " [0.3315162  0.33151206 0.33697173]\n",
      " [0.33098668 0.3311166  0.3378968 ]\n",
      " [0.33159426 0.33156228 0.33684343]\n",
      " [0.3312033  0.33125412 0.3375425 ]\n",
      " [0.33184308 0.33181563 0.33634132]\n",
      " [0.3307623  0.33093184 0.33830592]\n",
      " [0.33064616 0.33085358 0.33850023]\n",
      " [0.3322377  0.33221748 0.3355448 ]\n",
      " [0.33095866 0.33103788 0.33800346]\n",
      " [0.33121476 0.3312236  0.33756158]\n",
      " [0.3316179  0.33158633 0.33679578]\n",
      " [0.33235294 0.33233485 0.33531222]\n",
      " [0.33213398 0.33211187 0.3357542 ]\n",
      " [0.3312373  0.3312172  0.33754554]\n",
      " [0.332246   0.33222595 0.335528  ]\n",
      " [0.33073398 0.33082187 0.3384441 ]\n",
      " [0.332643   0.3326853  0.33467168]\n",
      " [0.33176622 0.33173734 0.33649647]\n",
      " [0.3326657  0.33265337 0.33468089]\n",
      " [0.33258396 0.33262986 0.33478615]\n",
      " [0.33321813 0.3332252  0.33355665]\n",
      " [0.3311582  0.33129063 0.3375512 ]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.3315851  0.33169174 0.33672312]\n",
      " [0.33164808 0.3317509  0.336601  ]\n",
      " [0.3325413  0.3325898  0.33486897]\n",
      " [0.3310388  0.33117843 0.33778277]\n",
      " [0.33124864 0.33137563 0.33737573]\n",
      " [0.33089584 0.3310441  0.33806002]\n",
      " [0.33090422 0.33105195 0.33804384]\n",
      " [0.33168986 0.33196747 0.3363426 ]\n",
      " [0.33130136 0.33142516 0.33727354]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33330804 0.33330962 0.33338237]\n",
      " [0.33329403 0.33329743 0.33340853]\n",
      " [0.33327028 0.33327416 0.3334556 ]\n",
      " [0.33325362 0.3332598  0.3334866 ]\n",
      " [0.33321398 0.3332213  0.33356476]\n",
      " [0.33320418 0.3332121  0.33358374]\n",
      " [0.33316386 0.33317426 0.33366188]\n",
      " [0.33319563 0.3332171  0.33358726]\n",
      " [0.3331678  0.33318803 0.33364418]\n",
      " [0.3332537  0.33330116 0.33344513]\n",
      " [0.33320293 0.33324793 0.33354917]\n",
      " [0.333236   0.33329657 0.33346742]\n",
      " [0.3331357  0.33317864 0.3336857 ]\n",
      " [0.33309108 0.3331283  0.33378062]\n",
      " [0.3331671  0.33323067 0.33360225]\n",
      " [0.33320314 0.33328396 0.33351293]\n",
      " [0.33301747 0.33311516 0.33386737]\n",
      " [0.3326552  0.33273092 0.33461392]\n",
      " [0.33298573 0.33309466 0.33391964]\n",
      " [0.33318588 0.3332846  0.33352953]\n",
      " [0.33291492 0.3330368  0.3340483 ]\n",
      " [0.3328657  0.33299443 0.33413985]\n",
      " [0.33253425 0.33266005 0.3348057 ]\n",
      " [0.33309832 0.3332228  0.3336789 ]\n",
      " [0.33246732 0.33260444 0.33492824]\n",
      " [0.3323856  0.33251795 0.33509642]\n",
      " [0.33232272 0.3324474  0.33522987]\n",
      " [0.332375   0.3325302  0.33509478]\n",
      " [0.33225262 0.33239    0.33535734]\n",
      " [0.3324288  0.33260477 0.33496645]\n",
      " [0.33216807 0.33230633 0.33552563]\n",
      " [0.33212683 0.33225957 0.3356136 ]\n",
      " [0.33229247 0.33233023 0.3353773 ]\n",
      " [0.3320585  0.33221376 0.33572778]\n",
      " [0.33255747 0.33254313 0.33489943]\n",
      " [0.3330142  0.3330083  0.3339775 ]\n",
      " [0.33198482 0.3321732  0.33584198]\n",
      " [0.33261627 0.33260304 0.33478063]\n",
      " [0.33285713 0.3328483  0.33429456]\n",
      " [0.33224013 0.3322434  0.33551642]\n",
      " [0.3324416  0.33242515 0.3351332 ]\n",
      " [0.33269045 0.3326786  0.33463097]\n",
      " [0.3326989  0.33268717 0.3346139 ]\n",
      " [0.33281258 0.33280295 0.33438447]\n",
      " [0.33208668 0.3320942  0.3358191 ]\n",
      " [0.33280307 0.33283558 0.33436137]\n",
      " [0.33272403 0.33276138 0.3345146 ]\n",
      " [0.33299828 0.33299205 0.33400968]\n",
      " [0.33264238 0.3326296  0.334728  ]\n",
      " [0.3326594  0.3327007  0.33463994]\n",
      " [0.3331876  0.33319655 0.33361587]\n",
      " [0.33261853 0.3326623  0.33471915]\n",
      " [0.332757   0.33279234 0.33445066]\n",
      " [0.33214498 0.33221763 0.33563742]\n",
      " [0.332058   0.33213595 0.33580604]\n",
      " [0.33259934 0.3326443  0.33475634]\n",
      " [0.33288527 0.33291274 0.334202  ]\n",
      " [0.3328757  0.33290374 0.33422053]\n",
      " [0.33183804 0.33192933 0.33623263]\n",
      " [0.3323348  0.33239588 0.33526936]\n",
      " [0.33191535 0.33200192 0.33608276]\n",
      " [0.33185872 0.33194876 0.33619252]\n",
      " [0.3317341  0.3318317  0.33643422]\n",
      " [0.33223137 0.33229876 0.33546984]\n",
      " [0.33177692 0.33187193 0.33635116]\n",
      " [0.33195767 0.33204168 0.33600068]\n",
      " [0.33183306 0.33192465 0.3362423 ]\n",
      " [0.3317378  0.33183518 0.33642703]\n",
      " [0.33196384 0.33207366 0.33596247]\n",
      " [0.33216667 0.3323674  0.33546597]\n",
      " [0.33178365 0.33187824 0.33633807]\n",
      " [0.33185354 0.33227196 0.3358745 ]\n",
      " [0.33248958 0.33279195 0.33471844]\n",
      " [0.33197537 0.3321432  0.33588138]\n",
      " [0.3322133  0.3324662  0.33532047]\n",
      " [0.33262613 0.33301693 0.33435696]\n",
      " [0.33267447 0.33303285 0.3342927 ]\n",
      " [0.3316367  0.3317402  0.33662307]\n",
      " [0.33043    0.33082396 0.3387461 ]\n",
      " [0.3322955  0.33272853 0.33497593]\n",
      " [0.33073133 0.33119565 0.33807305]\n",
      " [0.33253095 0.33295566 0.3345134 ]\n",
      " [0.3302108  0.33058387 0.33920535]\n",
      " [0.33074698 0.33123517 0.33801785]\n",
      " [0.3303225  0.33077574 0.33890173]\n",
      " [0.3318071  0.33229643 0.33589646]\n",
      " [0.33008122 0.3303814  0.33953738]\n",
      " [0.33000755 0.33037415 0.33961827]\n",
      " [0.33022955 0.33042318 0.3393473 ]\n",
      " [0.3302997  0.33080223 0.33889806]\n",
      " [0.33004197 0.33051184 0.33944616]\n",
      " [0.33188456 0.3318579  0.33625758]\n",
      " [0.33007166 0.3305732  0.3393551 ]\n",
      " [0.32983804 0.3301469  0.34001505]\n",
      " [0.3311187  0.33107805 0.33780318]\n",
      " [0.33014986 0.33030242 0.33954775]\n",
      " [0.3308996  0.33085915 0.33824122]\n",
      " [0.33000377 0.33018512 0.3398111 ]\n",
      " [0.33107474 0.3310333  0.33789194]]\n"
     ]
    }
   ],
   "source": [
    "#adding activation function\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "class layer_dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.10 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "class Activation_ReLU:  \n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "        \n",
    "class Activation_softmax:\n",
    "    def forward(self, inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilities\n",
    "\n",
    "X, y = spiral_data(samples=100,classes= 3)        \n",
    "        \n",
    "dense1 = layer_dense(2, 3)  # object1\n",
    "Activation1 = Activation_ReLU()\n",
    "\n",
    "dense2 = layer_dense(3,3)  # object1\n",
    "Activation2 = Activation_softmax()\n",
    "\n",
    "dense1.forward(X)\n",
    "Activation1.forward(dense1.output)\n",
    "\n",
    "dense2.forward(Activation1.output)\n",
    "Activation2.forward(dense2.output)\n",
    "\n",
    "print(Activation2.output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "769e66ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6486586255873816\n",
      "5.199999999999999\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "b=5.2\n",
    "\n",
    "print(np.log(b))\n",
    "print(math.e**1.6486586255873816)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ae2752b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35667494393873245\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "softmax_output=[0.7,0.4,2.3]\n",
    "target_output=[1,0,0]\n",
    "target_class = 0\n",
    "loss=-(math.log(softmax_output[0])*target_output[0]+\n",
    "       math.log(softmax_output[1])*target_output[1]+\n",
    "       math.log(softmax_output[2])*target_output[2])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93466bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.33331734 0.33331832 0.33336434]\n",
      " [0.3332888  0.33329153 0.33341965]\n",
      " [0.33325943 0.33326396 0.33347666]\n",
      " [0.33323312 0.33323926 0.33352762]]\n",
      "Loss: 1.098445\n"
     ]
    }
   ],
   "source": [
    "#adding activation function\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.10 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "class Activation_ReLU:  \n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "        \n",
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilities\n",
    "        \n",
    "        \n",
    "        \n",
    "class Loss:\n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output, y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        return data_loss\n",
    "\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped*y_true, axis=1)\n",
    "\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "\n",
    "    \n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "dense1 = Layer_Dense(2,3)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "dense1.forward(X)\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "dense2.forward(activation1.output)\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "print(activation2.output[:5])\n",
    "\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "loss = loss_function.calculate(activation2.output, y)\n",
    "\n",
    "print(\"Loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3fe358",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
